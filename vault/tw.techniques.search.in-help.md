---
id: j66tib0su6bhuz53btv83b3
title: In Help
desc: ''
updated: 1653329802418
created: 1652953712280
---

# In-help search 

It can be hard to find relevant information in large documentations, that’s why most of the users use search. 
We are all children of the web now, and we come to any information system looking for the search box, and expecting the search to work like Google. 

## Heatmap

If you look at the heat map we bet you’ll see many clicks and interactions around the search query area and less around the TOC. 

## EPPO

https://idratherbewriting.com/2011/05/16/every-page-is-page-one/ 

## Metrics 

- Precision
- Recall
- Accuracy
- Zero searches rate
- Search page exits

Crowdsorcing


## Synonims

“Design vocabulary” used by the technical communicator to build the search system doesn’t match the search vocabulary the user is possessing.

Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance. 

Precision is the estimated probability that a document randomly selected from the pool of retrieved documents is relevant.

Recall is the estimated probability that a document randomly selected from the pool of relevant documents is retrieved.

Precision
In the field of information retrieval, precision is the fraction of retrieved documents that are relevant to the query:

{\displaystyle {\text{precision}}={\frac {|\{{\text{relevant documents}}\}\cap \{{\text{retrieved documents}}\}|}{|\{{\text{retrieved documents}}\}|}}}{\displaystyle {\text{precision}}={\frac {|\{{\text{relevant documents}}\}\cap \{{\text{retrieved documents}}\}|}{|\{{\text{retrieved documents}}\}|}}}
For example, for a text search on a set of documents, precision is the number of correct results divided by the number of all returned results.

Precision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called precision at n or P@n.

Precision is used with recall, the percent of all relevant documents that is returned by the search. The two measures are sometimes used together in the F1 Score (or f-measure) to provide a single measurement for a system.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and technology.

Recall
In information retrieval, recall is the fraction of the relevant documents that are successfully retrieved.

{\displaystyle {\text{recall}}={\frac {|\{{\text{relevant documents}}\}\cap \{{\text{retrieved documents}}\}|}{|\{{\text{relevant documents}}\}|}}}{\displaystyle {\text{recall}}={\frac {|\{{\text{relevant documents}}\}\cap \{{\text{retrieved documents}}\}|}{|\{{\text{relevant documents}}\}|}}}
For example, for a text search on a set of documents, recall is the number of correct results divided by the number of results that should have been returned.

In binary classification, recall is called sensitivity. It can be viewed as the probability that a relevant document is retrieved by the query.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough. One needs to measure the number of non-relevant documents also, for example by also computing the precision.
